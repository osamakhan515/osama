{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = twitter_sentiment_tacosushi\n",
    "#Import the necessary methods from tweepy library as well as json and time\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#We authenticate ourselves as having a twitter app\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"3ads;fkajsdfpoaisdjf;alksdjf\"\n",
    "access_secret = \"2uasdl;fajsd;flkjasd;flkajsdf;adfasdfEg\"\n",
    "consumer_key = \"xasdf98uoif;wqjer;kandsf\"\n",
    "consumer_secret = \"1asd9fp8uijl;qkwef;alksd;iX\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#We begin searching our query\n",
    "#Put your search term\n",
    "searchquery = \"angry\"\n",
    "\n",
    "users =tweepy.Cursor(api.search,q=searchquery).items()\n",
    "count = 0\n",
    "start = 0\n",
    "errorCount=0\n",
    "\n",
    "#We will be storing our data in file called: happy.json\n",
    "#file = open('test.json', 'wb') \n",
    "\n",
    "#here we tell the program how fast to search \n",
    "waitquery = 100      #this is the number of searches it will do before resting\n",
    "waittime = 2.0          # this is the length of time we tell our program to rest\n",
    "total_number = 15000     #this is the total number of queries we want\n",
    "justincase = 1         #this is the number of minutes to wait just in case twitter throttles us\n",
    "\n",
    "\n",
    "\n",
    "text = [0] * total_number\n",
    "secondcount = 0\n",
    "idvalues = [1] * total_number\n",
    " #1 is happy; 2 is sad; 3 is angry; 4 is fearful\n",
    "#Below is where the magic happens and the queries are being made according to our desires above\n",
    "while secondcount < total_number:\n",
    "    try:\n",
    "        user = next(users)\n",
    "        count += 1\n",
    "        \n",
    "        #We say that after every 100 searches wait 5 seconds\n",
    "        if (count%waitquery == 0):\n",
    "           time.sleep(waittime)\n",
    "            #break\n",
    "\n",
    "    except tweepy.TweepError:\n",
    "        #catches TweepError when rate limiting occurs, sleeps, then restarts.\n",
    "        #nominally 15 minnutes, make a bit longer to avoid attention.\n",
    "        print \"sleeping....\"\n",
    "        time.sleep(60*justincase)\n",
    "        user = next(users)\n",
    "        \n",
    "        \n",
    "    except StopIteration:\n",
    "        break\n",
    "    try:\n",
    "        #print \"Writing to JSON tweet number:\"+str(count)\n",
    "        text_value = user._json['text']\n",
    "        language = user._json['lang']\n",
    "        #print(text_value)\n",
    "        print(language)\n",
    "        \n",
    "        if \"RT\" not in text_value:\n",
    "            if language == \"en\":\n",
    "                text[secondcount] = text_value\n",
    "                secondcount = secondcount + 1\n",
    "                print(\"current saved is:\")\n",
    "                print(secondcount)\n",
    "\n",
    "    except UnicodeEncodeError:\n",
    "        errorCount += 1\n",
    "        print \"UnicodeEncodeError,errorCount =\"+str(errorCount)\n",
    "\n",
    "\n",
    "print(\"Creating dataframe:\")\n",
    "\n",
    "d = {\"text\": text, \"id\": idvalues}\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "df.to_csv('upset.csv', header=True, index=False, encoding='utf-8')\n",
    "\n",
    "print \"completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import string\n",
    "\n",
    "#import my csv file\n",
    "df = pd.read_csv('unsmile.csv')\n",
    "\n",
    "#Remove any rows with a \"nan\" in them\n",
    "df = df.dropna(axis=0, how = 'any')\n",
    "\n",
    "#Make it so that any non readable text gets converted into nothing\n",
    "def removetext(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "#Here I am doing the actual removing\n",
    "df['text'] = df['text'].apply(removetext)\n",
    "\n",
    "#Make all my texts lower case\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "#Get rid of all weird punctuation and extra lines\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('.',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\\n',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('?',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('!',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\"',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(';',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('#',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('&amp',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(',',' '))\n",
    "\n",
    "#Here I get each unique keyword from my dataframe\n",
    "array = df['text'].str.split(' ', expand=True).stack().value_counts()\n",
    "#print(array) to see what this looks like\n",
    "\n",
    "#I make a dataframe of the words and the frequency with which the words appear \n",
    "d = {'word': array.index, 'frequency':array}\n",
    "df2 = pd.DataFrame(data = d)\n",
    "\n",
    "#I get rid of any words that are mentioned less than 10 times\n",
    "df2['frequency'] = df2['frequency'][df2['frequency'] > 10] \n",
    "\n",
    "#Remove any rows with a \"nan\" in them\n",
    "df2 = df2.dropna(axis=0, how = 'any')\n",
    "\n",
    "#Drop any obvious signs of these words being :(\n",
    "df2 = df2.drop([':(','https://t',':((', ':(((', ':((((', ':(((((', ':', '(', ''])\n",
    "\n",
    "#Convert my dataframe into a csv file\n",
    "df2.to_csv('unsmile_words.csv', header=True, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import my csv files\n",
    "happy = pd.read_csv('happy_words.csv')\n",
    "sad = pd.read_csv('sad_words.csv')\n",
    "unsmile = pd.read_csv('unsmile_words.csv')\n",
    "fun = pd.read_csv('fun_words.csv')\n",
    "\n",
    "\n",
    "wordbag = pd.concat([happy,sad,unsmile,fun]).drop_duplicates(subset = 'word').reset_index(drop=True)\n",
    "\n",
    "print(wordbag)\n",
    "\n",
    "wordbag.to_csv('wordbag.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('unsmile.csv')\n",
    "\n",
    "#Remove any rows with a \"nan\" in them\n",
    "df = df.dropna(axis=0, how = 'any')\n",
    "\n",
    "#Make it so that any non readable text gets converted into nothing\n",
    "def removetext(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "#Here I am doing the actual removing\n",
    "df['text'] = df['text'].apply(removetext)\n",
    "\n",
    "#Make all my texts lower case\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "#Get rid of all weird punctuation and extra lines\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('.',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\\n',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('?',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('!',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\"',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(';',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('#',' '))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(',',' '))\n",
    "\n",
    "#split all the words in my text\n",
    "df['text']= df['text'].str.split()\n",
    "\n",
    "df.to_csv('unsmile_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Read the files with the tweets\n",
    "fun = pd.read_csv('fun_split.csv')\n",
    "happy = pd.read_csv('happy_split.csv')\n",
    "unsmile = pd.read_csv('unsmile_split.csv')\n",
    "sad = pd.read_csv('sad_split.csv')\n",
    "\n",
    "#Get the wordbag\n",
    "wordbag = pd.read_csv('wordbag.csv')\n",
    "wordbag = wordbag.drop_duplicates()\n",
    "\n",
    "#Classify tweets that had the word fun or happy in them as 1 (positive)\n",
    "#and the others as 0 (negative)\n",
    "fun['type'] = 1\n",
    "happy['type'] = 1\n",
    "unsmile['type'] = 0\n",
    "sad['type'] = 0\n",
    "   \n",
    "#Join all of the dataframes into one big one for easier manipulation of a test/train split\n",
    "df = pd.concat([happy,sad,unsmile,fun]).reset_index(drop=True)\n",
    "\n",
    "#Create a test and train set by using the sklearn function train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Seperate the train data into a positive and negative set\n",
    "train_positive = train[train['type'] ==1]\n",
    "train_negative = train[train['type'] ==0]\n",
    "\n",
    "positive_instance = len(train_positive)\n",
    "negative_instance = len(train_negative)\n",
    "print(positive_instance)\n",
    "print(negative_instance)\n",
    "\n",
    "#Create your frequency table\n",
    "frequency['word'] = wordbag['word']\n",
    "\n",
    "word_bank = [0]*len(frequency)\n",
    "positive = [0]*len(frequency)\n",
    "negative = [0]*len(frequency)\n",
    "\n",
    "#Go over all the words in the frequency table\n",
    "for i in range(len(frequency)):\n",
    "    \n",
    "    #Get the word in the frequency table at a given row\n",
    "    word = frequency['word'].iloc[i]\n",
    "    word_bank[i] = word\n",
    "             \n",
    "    #Convert the word and attached single colons ont oboth sides of the word\n",
    "    check = str(\"'\") + word + str(\"'\")\n",
    "    \n",
    "    #Count the number of instances that have the word at least once\n",
    "    count = 0  \n",
    "    \n",
    "    #this iterates through each of the tweets in the positive train set\n",
    "    for j in range(len(train_positive)):\n",
    "        #This checks to see the number of time the said word appears in a given tweet\n",
    "        appears = train_positive['text'].iloc[j].count(check)\n",
    "        \n",
    "        #If the word appears at least once, we count it as that tweet having it\n",
    "        #We sum over all the tweets that the word appears at least once in \n",
    "        if appears > 0:\n",
    "            count = count + 1\n",
    "    positive[i] = count\n",
    "            \n",
    "    #Does the same thing but for negative numbers\n",
    "    count = 0  \n",
    "    for k in range(len(train_negative)):\n",
    "        appears = train_negative['text'].iloc[k].count(check)\n",
    "        if appears > 0:\n",
    "            count = count + 1\n",
    "    negative[i] = count\n",
    "    print(i)\n",
    "\n",
    "d = {'word': word_bank, 'positive': positive, 'negative': negative}\n",
    "ftable = pd.DataFrame(data = d)\n",
    "\n",
    "ftable.to_csv('ftable.csv')\n",
    "print(positive_instance)\n",
    "print(negative_instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "test = 'i am really happy birthday okay bob'\n",
    "\n",
    "#split all the words in my text\n",
    "test_words = test.str.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ftable = pd.read_csv('ftable.csv')\n",
    "ftable = ftable[ftable['word'] != 'sad']\n",
    "ftable = ftable[ftable['word'] != 'sad,']\n",
    "ftable = ftable[ftable['word'] != ':(']\n",
    "ftable = ftable[ftable['word'] != 'fun']\n",
    "ftable = ftable[ftable['word'] != 'happy,']\n",
    "ftable = ftable[ftable['word'] != 'happy']\n",
    "ftable = ftable.drop_duplicates(subset = 'word')\n",
    "\n",
    "test = 'i dont know what to do anymore'\n",
    "positive_instance = 24070.0\n",
    "negative_instance = 23930.0\n",
    "\n",
    "#split all the words in my text\n",
    "test_words = test.split()\n",
    "\n",
    "prob_positive = float(positive_instance/(positive_instance+negative_instance))\n",
    "prob_negative = 1 - prob_positive\n",
    "\n",
    "pos_word = 1.0*prob_positive\n",
    "neg_word = 1.0*prob_negative\n",
    "for i in range(len(test_words)):\n",
    "    word = test_words[i]\n",
    "    #print(word)\n",
    "    index_val = ftable.index[ftable['word'] == word]\n",
    "    if (len(index_val) > 0):\n",
    "        #print(index_val[0])\n",
    "        pos_val = ftable['positive'].iloc[index_val[0]]\n",
    "        neg_val = ftable['negative'].iloc[index_val[0]]\n",
    "        pos_word = pos_word * pos_val/positive_instance\n",
    "        neg_word = neg_word * neg_val/negative_instance\n",
    "        \n",
    "if pos_word > neg_word:\n",
    "    print(\"The sentence was POSITIVE, with a probability of\")\n",
    "    print(pos_word/(pos_word+neg_word))\n",
    "else:\n",
    "    print(\"The sentence was NEGATIVE, with a probability of\")\n",
    "    print(neg_word/(pos_word+neg_word))\n",
    "\n",
    "#print(pos_word)\n",
    "#print(neg_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
